<!DOCTYPE html>

<html lang="en" data-content_root="./">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>createllm &#8212; createllm 0.1.8 documentation</title>
    <link rel="stylesheet" type="text/css" href="_static/pygments.css?v=03e43079" />
    <link rel="stylesheet" type="text/css" href="_static/basic.css?v=686e5160" />
    <link rel="stylesheet" type="text/css" href="_static/alabaster.css?v=27fed22d" />
    <script src="_static/documentation_options.js?v=22607128"></script>
    <script src="_static/doctools.js?v=9bcbadda"></script>
    <script src="_static/sphinx_highlight.js?v=dc90522c"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
   
  <link rel="stylesheet" href="_static/custom.css" type="text/css" />
  

  
  

  </head><body>
  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          

          <div class="body" role="main">
            
  <section id="createllm">
<h1>createllm<a class="headerlink" href="#createllm" title="Link to this heading">¶</a></h1>
<p>A Python package that enables users to create and train their own Language Learning Models (LLMs) from scratch using custom datasets. This package provides a simplified approach to building, training, and deploying custom language models tailored to specific domains or use cases.</p>
<section id="core-purpose">
<h2>🎯 Core Purpose<a class="headerlink" href="#core-purpose" title="Link to this heading">¶</a></h2>
<p>createllm allows you to:</p>
<ul class="simple">
<li><p>Train custom language models on your specific text data</p></li>
<li><p>Create domain-specific LLMs for specialized applications</p></li>
<li><p>Build and experiment with different model architectures</p></li>
<li><p>Deploy trained models for text generation tasks</p></li>
</ul>
</section>
<section id="key-features">
<h2>✨ Key Features<a class="headerlink" href="#key-features" title="Link to this heading">¶</a></h2>
<ul class="simple">
<li><p>🔨 Build LLMs from scratch using your own text data</p></li>
<li><p>🚀 Efficient training with OneCycleLR scheduler</p></li>
<li><p>📊 Real-time training progress tracking with tqdm</p></li>
<li><p>🎛️ Configurable model architecture</p></li>
<li><p>💾 Easy model checkpointing and loading</p></li>
<li><p>🎯 Advanced text generation with temperature, top-k, and top-p sampling</p></li>
<li><p>📈 Built-in validation and early stopping</p></li>
<li><p>🔄 Automatic device selection (CPU/GPU)</p></li>
</ul>
</section>
<section id="requirements">
<h2>📋 Requirements<a class="headerlink" href="#requirements" title="Link to this heading">¶</a></h2>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>pip<span class="w"> </span>install<span class="w"> </span>createllm
</pre></div>
</div>
<p>The package requires:</p>
<ul class="simple">
<li><p>Python &gt;= 3.7</p></li>
<li><p>PyTorch &gt;= 2.0.0</p></li>
<li><p>tqdm &gt;= 4.65.0</p></li>
<li><p>numpy &gt;= 1.24.0</p></li>
<li><p>dataclasses &gt;= 0.6</p></li>
<li><p>typing-extensions &gt;= 4.5.0</p></li>
</ul>
</section>
<section id="quick-start-guide">
<h2>🚀 Quick Start Guide<a class="headerlink" href="#quick-start-guide" title="Link to this heading">¶</a></h2>
<section id="prepare-your-training-data">
<h3>1. Prepare Your Training Data<a class="headerlink" href="#prepare-your-training-data" title="Link to this heading">¶</a></h3>
<p>Place your training text in a file. The model learns from this text to generate similar content.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>my_training_data.txt
├── Your custom text
├── Can be articles
├── Documentation
└── Any text content you want the model to learn from
</pre></div>
</div>
</section>
<section id="train-your-custom-llm">
<h3>2. Train Your Custom LLM<a class="headerlink" href="#train-your-custom-llm" title="Link to this heading">¶</a></h3>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">createllm</span><span class="w"> </span><span class="kn">import</span> <span class="n">ModelConfig</span><span class="p">,</span> <span class="n">TextFileProcessor</span><span class="p">,</span> <span class="n">GPTLanguageModel</span><span class="p">,</span> <span class="n">GPTTrainer</span>

<span class="c1"># Initialize text processor with your data file</span>
<span class="n">processor</span> <span class="o">=</span> <span class="n">TextFileProcessor</span><span class="p">(</span><span class="s2">&quot;my_training_data.txt&quot;</span><span class="p">)</span>
<span class="n">text</span> <span class="o">=</span> <span class="n">processor</span><span class="o">.</span><span class="n">read_file</span><span class="p">()</span>

<span class="c1"># Tokenize the text</span>
<span class="n">train_data</span><span class="p">,</span> <span class="n">val_data</span><span class="p">,</span> <span class="n">vocab_size</span><span class="p">,</span> <span class="n">encode</span><span class="p">,</span> <span class="n">decode</span> <span class="o">=</span> <span class="n">processor</span><span class="o">.</span><span class="n">tokenize</span><span class="p">(</span><span class="n">text</span><span class="p">)</span>

<span class="c1"># Create model configuration</span>
<span class="n">config</span> <span class="o">=</span> <span class="n">ModelConfig</span><span class="p">(</span>
    <span class="n">vocab_size</span><span class="o">=</span><span class="n">vocab_size</span><span class="p">,</span>
    <span class="n">n_embd</span><span class="o">=</span><span class="mi">384</span><span class="p">,</span>      <span class="c1"># Embedding dimension</span>
    <span class="n">block_size</span><span class="o">=</span><span class="mi">256</span><span class="p">,</span>  <span class="c1"># Context window size</span>
    <span class="n">n_layer</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span>       <span class="c1"># Number of transformer layers</span>
    <span class="n">n_head</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span>        <span class="c1"># Number of attention heads</span>
    <span class="n">dropout</span><span class="o">=</span><span class="mf">0.2</span>      <span class="c1"># Dropout rate</span>
<span class="p">)</span>

<span class="c1"># Initialize the model</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">GPTLanguageModel</span><span class="p">(</span><span class="n">config</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Model initialized with </span><span class="si">{</span><span class="n">model</span><span class="o">.</span><span class="n">n_params</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="mf">1e6</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">M parameters&quot;</span><span class="p">)</span>

<span class="c1"># Initialize the trainer</span>
<span class="n">trainer</span> <span class="o">=</span> <span class="n">GPTTrainer</span><span class="p">(</span>
    <span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">,</span>
    <span class="n">train_data</span><span class="o">=</span><span class="n">train_data</span><span class="p">,</span>
    <span class="n">val_data</span><span class="o">=</span><span class="n">val_data</span><span class="p">,</span>
    <span class="n">config</span><span class="o">=</span><span class="n">config</span><span class="p">,</span>
    <span class="n">learning_rate</span><span class="o">=</span><span class="mf">3e-4</span><span class="p">,</span>
    <span class="n">batch_size</span><span class="o">=</span><span class="mi">64</span><span class="p">,</span>
    <span class="n">gradient_clip</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span>
    <span class="n">warmup_steps</span><span class="o">=</span><span class="mi">1000</span>
<span class="p">)</span>

<span class="c1"># Train the model</span>
<span class="n">trainer</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="n">max_epochs</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">save_dir</span><span class="o">=</span><span class="s1">&#39;checkpoints&#39;</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="generate-text-with-your-trained-model">
<h3>3. Generate Text with Your Trained Model<a class="headerlink" href="#generate-text-with-your-trained-model" title="Link to this heading">¶</a></h3>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Generate text</span>
<span class="n">context</span> <span class="o">=</span> <span class="s2">&quot;Once upon a time&quot;</span>
<span class="n">context_tokens</span> <span class="o">=</span> <span class="n">encode</span><span class="p">(</span><span class="n">context</span><span class="p">)</span>
<span class="n">context_tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="n">context_tokens</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">long</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>

<span class="n">generated</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span>
    <span class="n">context_tensor</span><span class="p">,</span>
    <span class="n">max_new_tokens</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span>
    <span class="n">temperature</span><span class="o">=</span><span class="mf">0.8</span><span class="p">,</span>
    <span class="n">top_k</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span>
    <span class="n">top_p</span><span class="o">=</span><span class="mf">0.9</span><span class="p">,</span>
    <span class="n">repetition_penalty</span><span class="o">=</span><span class="mf">1.2</span>
<span class="p">)</span>

<span class="c1"># Decode and print the generated text</span>
<span class="n">generated_text</span> <span class="o">=</span> <span class="n">decode</span><span class="p">(</span><span class="n">generated</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">tolist</span><span class="p">())</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Generated text:</span><span class="se">\n</span><span class="si">{</span><span class="n">generated_text</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</section>
</section>
<section id="example-use-cases">
<h2>📝 Example Use Cases<a class="headerlink" href="#example-use-cases" title="Link to this heading">¶</a></h2>
<section id="domain-specific-documentation-generator">
<h3>1. Domain-Specific Documentation Generator<a class="headerlink" href="#domain-specific-documentation-generator" title="Link to this heading">¶</a></h3>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Train on technical documentation</span>
<span class="n">processor</span> <span class="o">=</span> <span class="n">TextFileProcessor</span><span class="p">(</span><span class="s2">&quot;technical_docs.txt&quot;</span><span class="p">)</span>
<span class="n">text</span> <span class="o">=</span> <span class="n">processor</span><span class="o">.</span><span class="n">read_file</span><span class="p">()</span>
<span class="n">train_data</span><span class="p">,</span> <span class="n">val_data</span><span class="p">,</span> <span class="n">vocab_size</span><span class="p">,</span> <span class="n">encode</span><span class="p">,</span> <span class="n">decode</span> <span class="o">=</span> <span class="n">processor</span><span class="o">.</span><span class="n">tokenize</span><span class="p">(</span><span class="n">text</span><span class="p">)</span>

<span class="n">config</span> <span class="o">=</span> <span class="n">ModelConfig</span><span class="p">(</span><span class="n">vocab_size</span><span class="o">=</span><span class="n">vocab_size</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">GPTLanguageModel</span><span class="p">(</span><span class="n">config</span><span class="p">)</span>
<span class="n">trainer</span> <span class="o">=</span> <span class="n">GPTTrainer</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">train_data</span><span class="p">,</span> <span class="n">val_data</span><span class="p">,</span> <span class="n">config</span><span class="p">)</span>
<span class="n">trainer</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="n">max_epochs</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="custom-writing-style-model">
<h3>2. Custom Writing Style Model<a class="headerlink" href="#custom-writing-style-model" title="Link to this heading">¶</a></h3>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Train on specific author&#39;s works</span>
<span class="n">processor</span> <span class="o">=</span> <span class="n">TextFileProcessor</span><span class="p">(</span><span class="s2">&quot;author_works.txt&quot;</span><span class="p">)</span>
<span class="n">text</span> <span class="o">=</span> <span class="n">processor</span><span class="o">.</span><span class="n">read_file</span><span class="p">()</span>
<span class="n">train_data</span><span class="p">,</span> <span class="n">val_data</span><span class="p">,</span> <span class="n">vocab_size</span><span class="p">,</span> <span class="n">encode</span><span class="p">,</span> <span class="n">decode</span> <span class="o">=</span> <span class="n">processor</span><span class="o">.</span><span class="n">tokenize</span><span class="p">(</span><span class="n">text</span><span class="p">)</span>

<span class="n">config</span> <span class="o">=</span> <span class="n">ModelConfig</span><span class="p">(</span><span class="n">vocab_size</span><span class="o">=</span><span class="n">vocab_size</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">GPTLanguageModel</span><span class="p">(</span><span class="n">config</span><span class="p">)</span>
<span class="n">trainer</span> <span class="o">=</span> <span class="n">GPTTrainer</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">train_data</span><span class="p">,</span> <span class="n">val_data</span><span class="p">,</span> <span class="n">config</span><span class="p">)</span>
<span class="n">trainer</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="n">max_epochs</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
</pre></div>
</div>
</section>
</section>
<section id="model-configuration-options">
<h2>⚙️ Model Configuration Options<a class="headerlink" href="#model-configuration-options" title="Link to this heading">¶</a></h2>
<p>Customize your model architecture based on your needs:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">config</span> <span class="o">=</span> <span class="n">ModelConfig</span><span class="p">(</span>
    <span class="n">vocab_size</span><span class="o">=</span><span class="n">vocab_size</span><span class="p">,</span>  <span class="c1"># Vocabulary size from tokenization</span>
    <span class="n">n_embd</span><span class="o">=</span><span class="mi">384</span><span class="p">,</span>            <span class="c1"># Larger for more complex patterns</span>
    <span class="n">block_size</span><span class="o">=</span><span class="mi">256</span><span class="p">,</span>        <span class="c1"># Larger for longer context</span>
    <span class="n">n_layer</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span>             <span class="c1"># More layers for deeper understanding</span>
    <span class="n">n_head</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span>              <span class="c1"># More heads for better pattern recognition</span>
    <span class="n">dropout</span><span class="o">=</span><span class="mf">0.2</span>            <span class="c1"># Adjust for overfitting prevention</span>
<span class="p">)</span>
</pre></div>
</div>
</section>
<section id="training-tips">
<h2>💡 Training Tips<a class="headerlink" href="#training-tips" title="Link to this heading">¶</a></h2>
<section id="data-quality">
<h3>1. Data Quality<a class="headerlink" href="#data-quality" title="Link to this heading">¶</a></h3>
<ul class="simple">
<li><p>Clean your training data</p></li>
<li><p>Remove irrelevant content</p></li>
<li><p>Ensure consistent formatting</p></li>
</ul>
</section>
<section id="resource-management">
<h3>2. Resource Management<a class="headerlink" href="#resource-management" title="Link to this heading">¶</a></h3>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">trainer</span> <span class="o">=</span> <span class="n">GPTTrainer</span><span class="p">(</span>
    <span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">,</span>
    <span class="n">train_data</span><span class="o">=</span><span class="n">train_data</span><span class="p">,</span>
    <span class="n">val_data</span><span class="o">=</span><span class="n">val_data</span><span class="p">,</span>
    <span class="n">config</span><span class="o">=</span><span class="n">config</span><span class="p">,</span>
    <span class="n">batch_size</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span>     <span class="c1"># Reduce if running out of memory</span>
    <span class="n">learning_rate</span><span class="o">=</span><span class="mf">3e-4</span> <span class="c1"># Adjust based on your needs</span>
<span class="p">)</span>
</pre></div>
</div>
</section>
<section id="model-size-vs-performance">
<h3>3. Model Size vs Performance<a class="headerlink" href="#model-size-vs-performance" title="Link to this heading">¶</a></h3>
<ul class="simple">
<li><p>Smaller models (n_layer=4, n_head=4): Faster training, less complex patterns</p></li>
<li><p>Larger models (n_layer=8+, n_head=8+): Better understanding, more resource intensive</p></li>
</ul>
</section>
</section>
<section id="monitoring-training">
<h2>🔍 Monitoring Training<a class="headerlink" href="#monitoring-training" title="Link to this heading">¶</a></h2>
<p>The training process provides real-time feedback:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">Epoch</span> <span class="mi">1</span><span class="p">:</span> <span class="n">Training</span> <span class="n">Loss</span><span class="p">:</span> <span class="mf">3.1342</span><span class="p">,</span> <span class="n">Validation</span> <span class="n">Loss</span><span class="p">:</span> <span class="mf">4.3930</span>
<span class="n">Epoch</span> <span class="mi">2</span><span class="p">:</span> <span class="n">Training</span> <span class="n">Loss</span><span class="p">:</span> <span class="mf">2.3390</span><span class="p">,</span> <span class="n">Validation</span> <span class="n">Loss</span><span class="p">:</span> <span class="mf">4.5054</span>
<span class="n">Epoch</span> <span class="mi">3</span><span class="p">:</span> <span class="n">Training</span> <span class="n">Loss</span><span class="p">:</span> <span class="mf">2.0413</span><span class="p">,</span> <span class="n">Validation</span> <span class="n">Loss</span><span class="p">:</span> <span class="mf">4.5405</span>
<span class="n">Epoch</span> <span class="mi">4</span><span class="p">:</span> <span class="n">Training</span> <span class="n">Loss</span><span class="p">:</span> <span class="mf">1.9232</span><span class="p">,</span> <span class="n">Validation</span> <span class="n">Loss</span><span class="p">:</span> <span class="mf">4.5442</span>
<span class="n">Epoch</span> <span class="mi">5</span><span class="p">:</span> <span class="n">Training</span> <span class="n">Loss</span><span class="p">:</span> <span class="mf">1.8738</span><span class="p">,</span> <span class="n">Validation</span> <span class="n">Loss</span><span class="p">:</span> <span class="mf">4.5442</span>
</pre></div>
</div>
</section>
<section id="checkpoint-structure">
<h2>📁 Checkpoint Structure<a class="headerlink" href="#checkpoint-structure" title="Link to this heading">¶</a></h2>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>checkpoints/
├── checkpoint_epoch_0.pt  # Model checkpoint
├── checkpoint_epoch_1.pt
└── ...
</pre></div>
</div>
</section>
<section id="limitations">
<h2>⚠️ Limitations<a class="headerlink" href="#limitations" title="Link to this heading">¶</a></h2>
<ul class="simple">
<li><p>Training requires significant computational resources</p></li>
<li><p>Model quality depends on training data quality</p></li>
<li><p>Larger models require more training time and resources</p></li>
<li><p>Text generation quality may vary based on training data size and quality</p></li>
</ul>
</section>
<section id="contributing">
<h2>🤝 Contributing<a class="headerlink" href="#contributing" title="Link to this heading">¶</a></h2>
<p>Contributions are welcome! Please feel free to submit pull requests.</p>
</section>
<section id="support">
<h2>📫 Support<a class="headerlink" href="#support" title="Link to this heading">¶</a></h2>
<p>For issues and questions, please open an issue in the GitHub repository.</p>
</section>
<section id="license">
<h2>📄 License<a class="headerlink" href="#license" title="Link to this heading">¶</a></h2>
<p>This project is licensed under the MIT License.</p>
</section>
<section id="acknowledgments">
<h2>🙏 Acknowledgments<a class="headerlink" href="#acknowledgments" title="Link to this heading">¶</a></h2>
<p>Based on the GPT architecture with modifications for custom training and ease of use.</p>
</section>
</section>


          </div>
          
        </div>
      </div>
      <div class="sphinxsidebar" role="navigation" aria-label="Main">
        <div class="sphinxsidebarwrapper">
<h1 class="logo"><a href="index.html">createllm</a></h1>









<search id="searchbox" style="display: none" role="search">
    <div class="searchformwrapper">
    <form class="search" action="search.html" method="get">
      <input type="text" name="q" aria-labelledby="searchlabel" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" placeholder="Search"/>
      <input type="submit" value="Go" />
    </form>
    </div>
</search>
<script>document.getElementById('searchbox').style.display = "block"</script><h3>Navigation</h3>

<div class="relations">
<h3>Related Topics</h3>
<ul>
  <li><a href="index.html">Documentation overview</a><ul>
  </ul></li>
</ul>
</div>








        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="footer">
      &#169;2025, Khushal Jethava.
      
      |
      Powered by <a href="https://www.sphinx-doc.org/">Sphinx 8.1.3</a>
      &amp; <a href="https://alabaster.readthedocs.io">Alabaster 1.0.0</a>
      
      |
      <a href="_sources/readme.rst.txt"
          rel="nofollow">Page source</a>
    </div>

    

    
  </body>
</html>